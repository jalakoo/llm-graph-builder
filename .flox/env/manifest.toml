# manifest version managed by Flox CLI
version = 1

[install]
# frontend deps
nodejs.pkg-path = "nodejs"
yarn.pkg-path = "yarn"

# backend deps
python.pkg-path = "python311"
python.priority = 0

google-crc32c.pkg-path = "python311Packages.google-crc32c"

uv.pkg-path = "uv"
cmake.pkg-path = "cmake"

poppler_utils.pkg-path = "poppler_utils"
tesseract.pkg-path = "tesseract"

# System/C deps
gcc-unwrapped.pkg-path = "gcc-unwrapped"
pkg-config.pkg-path = "pkg-config"
glibc.pkg-path = "glibc"  # glibc only needed on linux builds
glibc.systems = [ "x86_64-linux" , "aarch64-linux"]

# db
neo4j.pkg-path = "neo4j"
ollama.pkg-path = "ollama"


[vars]
UV_PYTHON_PREFERENCE = "only-system"
UV_PYTHON_DOWNLOADS = "never"

# Backend
NEO4J_URI = "neo4j://localhost:7687"
NEO4J_PASSWORD = "password"
NEO4J_USERNAME = "neo4j"
OPENAI_API_KEY = ""
DIFFBOT_API_KEY = ""
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
LANGCHAIN_ENDPOINT = ""
LANGCHAIN_TRACING_V2 = ""
LANGCHAIN_PROJECT = ""
LANGCHAIN_API_KEY = ""
KNN_MIN_SCORE = "0.94"
IS_EMBEDDING = "true"
GEMINI_ENABLED = "False"
GCP_LOG_METRICS_ENABLED = "False"
UPDATE_GRAPH_CHUNKS_PROCESSED = "20"
NUMBER_OF_CHUNKS_TO_COMBINE = "6"
ENTITY_EMBEDDING = "False"
GCS_FILE_CACHE = "False"
# LLM_MODEL_CONFIG_anthropic_claude_35_sonnet = ""
# LLM_MODEL_CONFIG_fireworks_llama_v3_70b = ""
# LLM_MODEL_CONFIG_azure_ai_gpt_4o = ""
# LLM_MODEL_CONFIG_azure_ai_gpt_35 = ""
# LLM_MODEL_CONFIG_groq_llama3_70b = ""
# LLM_MODEL_CONFIG_bedrock_claude_3_5_sonnet = ""
# LLM_MODEL_CONFIG_fireworks_qwen_72b = ""
LLM_MODEL_CONFIG_ollama_llama3 = "llama3,http://localhost:11434"

# Frontend
VITE_BACKEND_API_URL = "http://localhost:8000"
VITE_REACT_APP_SOURCES="local,youtube,wiki,s3,web"
VITE_GOOGLE_CLIENT_ID = ""
VITE_BLOOM_URL = "https://workspace-preview.neo4j.io/workspace/explore?connectURL={CONNECT_URL}&search=Show+me+a+graph&featureGenAISuggestions=true&featureGenAISuggestionsInternal=true"
VITE_TIME_PER_PAGE = "50"
VITE_CHUNK_SIZE = "5242880"
VITE_LARGE_FILE_SIZE = "5242880"
VITE_ENV = "DEV"
VITE_CHAT_MODES = ""
VITE_BATCH_SIZE = "2"
VITE_LLM_MODELS = ""
# VITE_LLM_MODELS_PROD="openai_gpt_4o,openai_gpt_4o_mini,diffbot,gemini_1.5_flash"
VITE_LLM_MODELS_PROD="ollama_llama3"


[hook]
on-activate = '''
  # If we export this here, it can be used later in 'profiles.common/bash'
  export PYTHON_DIR="$FLOX_ENV_CACHE/python"

  test -d "${PYTHON_DIR}" \
    || uv venv "${PYTHON_DIR}" --allow-existing
  source "$PYTHON_DIR/bin/activate"
  
  uv pip install -r backend/requirements.txt

  yarn --cwd frontend install

  ollama list | grep "^llama3" || ollama pull llama3
'''


[profile]
bash = '''
source "$PYTHON_DIR/bin/activate"
'''
fish = '''
source "$PYTHON_DIR/bin/activate.fish"
'''
tcsh = '''
source "$PYTHON_DIR/bin/activate.csh"
'''
zsh = '''
source "$PYTHON_DIR/bin/activate"
'''


[services]
frontend.command = '''
cd frontend
yarn run dev
'''

backend.command = '''
source "$PYTHON_DIR/bin/activate"
cd backend
python3 score.py
'''
# gunicorn score:app --workers 8 --threads 8 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000 --timeout 300

ollama.command = '''
ollama serve
'''

[options]
systems = ["aarch64-darwin", "aarch64-linux", "x86_64-darwin", "x86_64-linux"]
# Uncomment to disable CUDA detection.
# cuda-detection = false

